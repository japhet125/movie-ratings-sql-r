---
title: "Penguin Predictions Analysis"
author: "Guibril Ramde"
date: "2026-02-05"
output: html_document
---

# Approach

In this assignment, I will analyze the performance of a binary classification model predicting penguin sex.  

The workflow will be as follows:

1. **Load the data**: I will download the raw CSV file from GitHub and load it into R.  
2. **Install and load required libraries**: Necessary libraries such as `ggplot2` and `dplyr` will be used for visualization and data manipulation.  
3. **Compute the null error rate**: This baseline metric shows the error if we always predict the majority class.  
4. **Visualize class distribution**: A plot will provide insight into the balance of the dataset and help interpret the null error rate.  
5. **Compute confusion matrices**: I will calculate TP, FP, TN, and FN at multiple probability thresholds.  
6. **Calculate performance metrics**: Accuracy, precision, recall, and F1 score will be computed for each threshold.  
7. **Discuss threshold use cases**: I will describe scenarios where different thresholds are preferable.  

The raw CSV file is located at:  
[Penguin Predictions CSV](https://raw.githubusercontent.com/acatlin/data/refs/heads/master/penguin_predictions.csv)

#Conclusion

tAssignment 2B was challenging and required careful attention to both the conceptual and practical aspects of classification model evaluation. In particular, reproducing predicted classes at different probability thresholds and manually computing the corresponding confusion matrices required a deeper understanding of how model outputs translate into performance metrics.

Overall, the identical performance metrics observed across multiple thresholds indicate that the modelâ€™s predicted probabilities are concentrated away from the threshold boundaries. As a result, adjusting the probability threshold does not change the predicted class labels, leading to identical confusion matrices and, consequently, identical accuracy, recall, and F1 scores. This outcome highlights the importance of examining the distribution of predicted probabilities when interpreting threshold-based evaluation results.